---
categories:  
- ""    #the front matter should be like the one found in, e.g., blog2.md. It cannot be like the normal Rmd we used
- ""
date: "2021-10-13"
description: Best Visualisations from Data Analytics # the title that will show up once someone gets to this page
draft: false
image: sky.jpg # save picture in \static\img\blogs. Acceptable formats= jpg, jpeg, or png . Your iPhone pics wont work

keywords: ""
slug: visuals # slug is the shorthand URL address... no spaces plz
title: My best visualisations from Data Analytics
---
  
  
```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(fivethirtyeight)
library(here)
library(skimr)
library(janitor)
library(vroom)
library(tidyquant)
library(scales)
library(zoo)
library(ggtext)
library(rvest) # to scrape wikipedia page
```

## Homework 1, Challenge 2: Opinion polls for the 2021 German elections

In this exercise, we were tasked with replicating a graph from the Guardian, which depicted the results of election polls prior to the 2021 German parliamentary election. The Guardian newspaper had an [election poll tracker for the upcoming German election](https://www.theguardian.com/world/2021/aug/20/german-election-poll-tracker-who-will-be-the-next-chancellor).The list of the opinion polls since Jan 2021 could be found at [Wikipedia](https://en.wikipedia.org/wiki/Opinion_polling_for_the_2021_German_federal_election)

First, we used the following code to scrape the Wikipedia page and import the table in a dataframe.



```{r, scrape_wikipedia_polling_data, warnings= FALSE, message=FALSE}
url <- "https://en.wikipedia.org/wiki/Opinion_polling_for_the_2021_German_federal_election"

# get tables that exist on Wikipedia page 
tables <- url %>% 
  read_html() %>% 
  html_nodes(css="table")


# parse HTML tables into a dataframe called polls 
# Use purr::map() to create a list of all tables in URL
polls <- map(tables, . %>% 
             html_table(fill=TRUE)%>% 
             janitor::clean_names())


# list of opinion polls
german_election_polls <- polls[[1]] %>% # the first table on the page contains the list of all opinions polls
  slice(2:(n()-1)) %>%  # drop the first row, as it contains again the variable names and last row that contains 2017 results
  mutate(
         # polls are shown to run from-to, e.g. 9-13 Aug 2021. We keep the last date, 13 Aug here, as the poll date
         # and we extract it by picking the last 11 characters from that field
         end_date = str_sub(fieldwork_date, -11),
         
         # end_date is still a string, so we convert it into a date object using lubridate::dmy()
         end_date = dmy(end_date),
         
         # we also get the month and week number from the date, if we want to do analysis by month- week, etc.
         month = month(end_date),
         week = isoweek(end_date)
         )
```

After downloading the data and formating it, I recreated the graph using the following code:

```{r, fig.height=6, fig.width=9}

# First, I use the following code to calculate the average result of the polls for each party by grouping the polls by their end date. The reason I do this is because there are many polls per day in some cases and I would like to crate a 14-day moving average in the following step

german_election_polls_means <- german_election_polls %>% 
  group_by (end_date) %>% 
  summarise(mean_union = mean(union, na.rm = TRUE), 
            mean_spd = mean(spd, na.rm = TRUE), 
            mean_afd = mean(af_d, na.rm = TRUE),
            mean_fdp = mean(fdp, na.rm = TRUE),
            mean_linke = mean(linke, na.rm = TRUE),
            mean_grune = mean(grune, na.rm = TRUE)
      )


# Next I: 
#     plot the data for all parties on the same graph using geom_point and name the graph "plot"
#     define the x-axis as the end date of the survey
#     assign each party a color and make the points see through
#     use rollmean to calculate rolling 14 day average to show the trend per party

plot <- ggplot(german_election_polls, 
               aes(x = end_date)) + 
  geom_point(aes(y = union), 
             color = "#000000", 
             alpha = 0.3) +
  
  geom_line(data = german_election_polls_means, 
            aes(y = rollmean(mean_union, 
                             14, na.pad = TRUE)), 
            size = 1, 
            color = "#000000") +

  geom_point(aes(y = spd), 
             color = "#FF0000", 
             alpha = 0.3) +
  
  geom_line(data = german_election_polls_means, 
            aes(y = rollmean(mean_spd, 14, na.pad = TRUE)), 
            size = 1, 
            color = "#FF0000") +

  geom_point(aes(y = af_d), 
             color = "#0080FF", 
             alpha = 0.3) +
  
  geom_line(data = german_election_polls_means, 
            aes(y = rollmean(mean_afd, 14, na.pad = TRUE)), 
            size = 1, 
            color = "#0080FF") +

  geom_point(aes(y = fdp), 
             color = "#FFFF00", 
             alpha = 0.3) +
  
  geom_line(data = german_election_polls_means, 
            aes(y = rollmean(mean_fdp, 14, na.pad = TRUE)), 
            size = 1, 
            color = "#FFFF00") +

  geom_point(aes(y = linke), 
             color = "#A23FAE", 
             alpha = 0.3) +
  
  geom_line(data = german_election_polls_means, 
            aes(y = rollmean(mean_linke, 14, na.pad = TRUE)), 
            size = 1, 
            color = "#A23FAE") +

  
  geom_point(aes(y = grune), 
             color = "#006633", 
             alpha = 0.3) +
  
  geom_line(data = german_election_polls_means, 
            aes(y = rollmean(mean_grune, 14, na.pad = TRUE)), 
            size = 1, 
            color = "#006633")


# Finally, I use the following chunk of code to format the graph to match the one from the Guardian

  plot + scale_y_continuous("", 
                            expand = c(0,5), 
                            breaks = seq(0, 45, by = 10),
                            minor_breaks = seq(0, 50, by = 5),
                            labels = label_number(suffix = "%") ) +
          scale_x_date("", 
                       expand = c(0,20), 
                       date_breaks = "2 month", 
                       date_minor_breaks = "1 month", 
                       date_labels = "%b %Y") + 
          coord_fixed(4) +
          labs(
                  title = "German Election Polling Timeseries",
                  caption = "Data from: 'https://en.wikipedia.org/wiki/Opinion_polling_for_the_2021_German_federal_election'"
      
              )


```

In retrospect, it would have been better to convert the data to long format instead of specifying aesthetics for each party individually. Let's try to do this with the following code:

```{r, fig.height=6, fig.width=9}
# Creating a new dataframe that will include the long data
german_election_polls_long <- german_election_polls %>%
# Specifying the variables that we want to turn long
  pivot_longer(c("union", 
                 "spd", 
                 "af_d", 
                 "fdp", 
                 "linke", 
                 "grune"),
               # Creating a new variable "party" that will take the names of the parties
               names_to = "party",
               # Creating a new variable "votes" to take the polling values fo each party
               values_to = "votes")

# Next, we recreate the same graph using the new data
german_election_polls_long %>%
  # We group the votes by party
  group_by(party) %>% 
  ggplot(aes(x = end_date,
             y = votes,
             color = party))+
  geom_point()+
  geom_smooth(se = FALSE,
              na.rm = TRUE)+
  scale_color_manual(
    values = c("#000000",
               "#FF0000",
               "#0080FF",
               "#FFFF00",
               "#A23FAE",
               "#006633"),
    breaks = c("union",
               "spd",
               "af_d",
               "fdp",
               "linke",
               "grune"),
    labels = c("Union",
               "SPD",
               "AfD",
               "FDP",
               "Linke",
               "Grune")
  )+
  scale_y_continuous("", 
                     expand = c(0,5), 
                     breaks = seq(0, 45, by = 10),
                     minor_breaks = seq(0, 50, by = 5),
                     labels = label_number(suffix = "%") ) +
  scale_x_date("", 
               expand = c(0,20), 
               date_breaks = "2 month", 
               date_minor_breaks = "1 month", 
               date_labels = "%b %Y") + 
          labs(
               title = "German Election Polling Timeseries",
               caption = "Data from: 'https://en.wikipedia.org/wiki/Opinion_polling_for_the_2021_German_federal_election'"
      
              )


```

Ok, we didn't bother with the moving average, but we included a legend this time. And the code took around 30 lines less to complete.

It is interesting looking at this graph now, post election. As we saw a few weeks ago, the SPD did win and dethroned the long-standing party of Angela Merkel, which means the polls were right and the shift we observe in September was accurate. Now, we can only watch the news and see if the "Trafic Light" coalition between SPD, FDP, and Grune will materialize. 

## Homework 2, Challenge 2: How has the CPI and its components changed over the last few years?

In this next challenge, my group mates Otto, Yuna, and I had to pull data on the components of CPI from the Federal Reserve Economic Data (FRED) website and recreate a graph.

We find the date on [CPI components at  FRED](https://fredaccount.stlouisfed.org/public/datalist/843)

First, we pull the data and format it with the following code:

```{r, warning = FALSE, message =FALSE}
url <- "https://fredaccount.stlouisfed.org/public/datalist/843"


# We get the tables with CPI data that exist on FRED page 
tables <- url %>% 
  read_html() %>% 
  html_nodes(css="table")


# We parse the HTML tables into a dataframe called cpis 
# We use purr::map() to create a list of all tables in URL
cpis <- map(tables, . %>% 
             html_table(fill=TRUE)%>% 
             janitor::clean_names())

cpi_id <- cpis[[2]] %>% # the second table on the page contains the list of all cpi components
  select(series_id)

vectorcpi_id <- as.vector(t(cpi_id)) # We transform the dataframe into vector form
  
cpi_data <- tidyquant::tq_get(vectorcpi_id, get = "economic.data", from =  "2000-01-01") # We extract data from the FRED website 

cpi_names <- cpis[[2]] # We create a different dataframe that includes the observation titles called cpi_names

cpi_doc <- left_join(cpi_data, cpi_names,
                     by = c("symbol" = "series_id")) # We merge the data and the titles dataframes

# We use the lag function to get the 12 month change in prices for each component ...
cpi_change <- cpi_doc %>%
  group_by(title) %>% 
  mutate(year_change = price/lag(price, 12, na.rm = TRUE) - 1,
            date) %>% 
  na.omit()
  
# ... and make sure that "All Items" appears first in the dataframe
# Additionally, we clean the titles
cpi_change <- cpi_change %>% 
  mutate(index = symbol == "CPIAUCSL") %>%
  mutate(title = str_remove_all(title, "Consumer Price Index for All Urban Consumers: ")) %>%
  mutate(title = str_remove_all(title, " in U.S. City Average"))

# Next, we order the components within each month based on their impact on the annual CPI change for that month and create a new dataframe
cpi_ordered <- cpi_change %>%
    group_by(date) %>%
  arrange(desc(index), date, desc(year_change))

cpi_ordered
```


```{r, warnings= FALSE, message=FALSE, fig.height=8, fig.width=10}
# Next, we create the scatter plot of each component, where a negative change is shown in blue and a positive change in CPI is shown in red
# We include the geom_smooth function to show the trend in CPI development
 cpi_ordered %>% 
  filter(date >= "2016-01-01") %>% 
  ggplot(aes(x = date, 
             y = year_change)) +
  geom_point(aes(color = year_change > 0)) +
  geom_smooth(colour = "dark grey", se=F) +
  facet_wrap(~title, 
             scales = "free") + 
  scale_y_continuous(labels=scales::percent) +
  labs(
       title="Yearly change of US CPI (All Items) and its components",
       subtitle="YoY change being <span style = 'color: brown1;'>positive</span> or <span style = 'color: darkturquoise;'>negative</span> \nJan 2016 to Aug 2021",
       y = "YoY % Change",
       x = "",
       caption = "Data from St. Louis Fed FRED \nhttps://fredaccount.stlouisfed.org/public/datalist/843"
       ) +
  theme_bw() +
  theme(plot.subtitle = element_markdown(),
        plot.caption = element_text(color="black"),
        legend.position = "none")
   

```

Some interesting observations can be made by looking at the disaggregated CPI components. For example, we can see the huge increase in Cars and Trucks. This is likely due to the global chip shortage after the pandemic. Other related items ike fuel and air travel are also large contributors to the CPI growth. 

## Homework 3, Challenge 1: Yield Curve inversion

In this assignment, I created 3 beautiful visualizations of US Treasury securities in order to gain a better understanding of the yield curve and what its inversion might mean.

Every so often, we hear warnings from commentators on the "inverted yield curve" and its predictive power with respect to recessions. An explination of what a [inverted yield curve is can be found here](https://www.reuters.com/article/us-usa-economy-yieldcurve-explainer/explainer-what-is-an-inverted-yield-curve-idUSKBN1O50GA). If you'd rather listen to something, here is a great podcast from [NPR on yield curve indicators](https://www.podbean.com/media/share/dir-4zgj9-6aefd11)

In addition, many articles and commentators think that, e.g., [*Yield curve inversion is viewed as a harbinger of recession*](https://www.bloomberg.com/news/articles/2019-08-14/u-k-yield-curve-inverts-for-first-time-since-financial-crisis). One can always doubt whether inversions are truly a harbinger of recessions, and [use the attached parable on yield curve inversions](https://twitter.com/5_min_macro/status/1161627360946511873).

In our case we will look at US data and use the [FRED database](https://fred.stlouisfed.org/) to download historical yield curve rates, and plot the yield curves since 1999 to see when the yield curves flatten. To know more, an article that explains the [yield curve is and its inversion can be found here](https://fredblog.stlouisfed.org/2018/10/the-data-behind-the-fear-of-yield-curve-inversions/). 

First, we will load the yield curve data file that contains data on the yield curve since 1960-01-01:

```{r download_historical_yield_curve, warning=FALSE, message=FALSE}

yield_curve <- read_csv(here::here("data", "yield_curve.csv"))

glimpse(yield_curve)
```

Our dataframe `yield_curve` has five columns (variables):

- `date`: already a date object
- `series_id`: the FRED database ticker symbol
- `value`: the actual yield on that date
- `maturity`: a short hand for the maturity of the bond
- `duration`: the duration, written out in all its glory!

### Yields on US rates by duration since 1960

```{r, fig.width= 15, fig.height= 10, warning=FALSE, message=FALSE}
yield_curve %>% 
  group_by(maturity) %>% 
  ggplot(aes(x = date,
             y = value,
             color = maturity))+
  # We use this code to order the graphs based on maturity; We also specify that we want 2 columns 
    facet_wrap(~ factor(duration, 
                        levels = c("3-Month Treasury Bill",
                                   "6-Month Treasury Bill",
                                   "1-Year Treasury Rate",
                                   "2-Year Treasury Rate",
                                   "3-Year Treasury Rate",
                                   "5-Year Treasury Rate",
                                   "7-Year Treasury Rate",
                                   "10-Year Treasury Rate",
                                   "20-Year Treasury Rate",
                                   "30-Year Treasury Rate")), 
               ncol = 2)+
    geom_line()+
    theme_bw()+
    theme(legend.position = "none")+
    labs(title="Yields on U.S. Tresury Rates since 1960", 
       y = "%",
       x = "",
       caption = "Source: St. Louis Federal reserve Economic Database (FRED)"
       ) +
  NULL
```


### Monthly yields on US rates by duration since 1999 on a year-by-year basis

```{r, fig.width= 15, fig.height= 10, warning=FALSE, message=FALSE}

yield_curve %>%
  #We create 2 new variables that take the month and year from the date variable respectively
  mutate(month = month(date),
         year = year(date)) %>%
  # We filter for date only after 1999
  filter(year >= 1999) %>%
  # We use levels to order the x-axis based on maturity
  ggplot(aes(x = factor(maturity, 
                        level = c("3m", 
                                  "6m", 
                                  "1y", 
                                  "2y", 
                                  "3y",
                                  "5y",
                                  "7y",
                                  "10y", 
                                  "20y", 
                                  "30y")),
             y = value))+
    facet_wrap(~ year, 
               ncol = 4)+
  # We use as.factor to color each graph based on the years
    geom_line(aes(group = month,
                  color = as.factor(year)))+
    theme_bw()+
    theme(legend.position = "none")+
    labs(title="US Yield Curve", 
       y = "Yield (%)",
       x = "Maturity",
       caption = "Source: St. Louis Federal reserve Economic Database (FRED)"
       ) +
  NULL
```


### 3-month and 10-year yields since 1999

```{r, fig.width= 15, fig.height= 10}

yield_curve %>%
  mutate(year = year(date)) %>%
  filter(year >= 1999) %>%
  #We filter for bonds that only have a 3-month and 10-year maturity
  filter( maturity == "10y" | maturity == "3m") %>% 
  group_by(maturity) %>% 
  ggplot(aes(x = date,
             y = value,
             color = duration))+
    geom_line()+
    theme_bw()+
  #We use this code to swap the colors of the two lines to match the original graph
    scale_color_hue(direction = -1, h.start=90)+
    labs(title="Yields on 3-Month and 10-Year Tresury rates since 1999", 
         x = "",
         y = "%",
       caption = "Source: St. Louis Federal reserve Economic Database (FRED)",
       color = "") +
  NULL
```


According to [Wikipedia's list of recession in the United States](https://en.wikipedia.org/wiki/List_of_recessions_in_the_United_States), since 1999 there have been two recession in the US: between Mar 2001–Nov 2001 and between Dec 2007–June 2009. In fact, we can see that the yield curve seems to flatten at those times since in both cases, the 10-year and the 3-month yield curves cross each other, suggesting the spread is equal to 0. We can see this happening in three places: around 2001 (Dot Com Bubble), around 2008 (The MBS Financial Crisis), and most recently around 2020 (The COVID-19 Pandemic). This can be explained by the fact that in anticipation of a recession and lower interest rate , investors buy long-term bonds and sell short-term ones, which affects their price and in turn, their yield. The evidence in the graph would suggest that the inversion of the yield curve is a good predictor of recessions.
